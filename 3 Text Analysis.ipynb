{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dense-lease",
   "metadata": {},
   "source": [
    "# Text Analysis on gurlitt data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "latin-marking",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-insurance",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\論文\\PHEME9\\Data\\CSV\\gurlitt.csv', encoding = 'utf-8', header = None)\n",
    "df.columns = ['src_tweet_id', 'src_user_id', 'src_tweet','src_date','reply_tweet_id','reply_user_id','reply_tweet','reply_date', 'label']\n",
    "df.reply_tweet = df.reply_tweet.replace(np.nan,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corporate-breakfast",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tw_df = df.iloc[:,[0,1,2,3,8]].copy()\n",
    "src_tw_df = src_tw_df.drop_duplicates()\n",
    "src_tw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "comprehensive-local",
   "metadata": {},
   "source": [
    "## Prepocessing\n",
    "- Define function to process text at one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decent-master",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textprocessing(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = \" \".join([word for word in text.split() if 'http' not in word\n",
    "                                and not word.startswith('@')])\n",
    "                                #and word != 'RT'])\n",
    "    import re\n",
    "    def remove_punct(tweet):\n",
    "        new_words = []\n",
    "        for word in tweet:\n",
    "            w = re.sub(r'[^\\w\\s]','',word) #remove everything except words and space\n",
    "            w = re.sub(r'_','',w) #how to remove underscore as well\n",
    "            new_words.append(w)\n",
    "\n",
    "        return new_words\n",
    "    text = \"\".join(remove_punct(text))\n",
    "          \n",
    "    from nltk.tokenize import TweetTokenizer\n",
    "    tknzr = TweetTokenizer(strip_handles=True)\n",
    "    text = tknzr.tokenize(text)\n",
    "    \n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    stem = PorterStemmer()\n",
    "    text = [stem.stem(i) for i in text]\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words=set(stopwords.words(\"english\"))\n",
    "    def filterstopwords(tw):\n",
    "        filter_stopwords = []\n",
    "        for w in tw:\n",
    "            if w not in stop_words:\n",
    "                filter_stopwords.append(w)\n",
    "        return filter_stopwords\n",
    "    text = filterstopwords(text)\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "institutional-riding",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"original text: \", df.reply_tweet.iloc[107])\n",
    "print(\"cleaned text: \", textprocessing(df.reply_tweet.iloc[107]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "metallic-ensemble",
   "metadata": {},
   "source": [
    "### applied on original data (**df**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "corrected-politics",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_src_tw'] = df.src_tweet.apply(textprocessing)\n",
    "df['cleaned_reply_tw'] = df.reply_tweet.apply(textprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "legendary-organ",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "contemporary-heaven",
   "metadata": {},
   "source": [
    "### applied on source tweet data (**src_tw_df**) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recreational-covering",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tw_df['cleaned_src_tw'] = src_tw_df.src_tweet.apply(textprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "virgin-kingston",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "small-spain",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ancient-abuse",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(tweet):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = analyzer.polarity_scores(tweet)\n",
    "    if vs['compound'] >=0.05:\n",
    "        sentiment_label = 'Positive'\n",
    "    elif (vs['compound'] > -0.05) & (vs['compound'] < 0.05):\n",
    "        sentiment_label = 'Neutral'\n",
    "    elif vs['compound']<= -0.05:\n",
    "        sentiment_label = 'Negative' \n",
    "    result = sentiment_label\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "organizational-swimming",
   "metadata": {},
   "source": [
    "### applied on original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "protecting-madonna",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['src_sentiment']= df.cleaned_src_tw.apply(get_sentiment)\n",
    "df['reply_sentiment']= df.cleaned_reply_tw.apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "worthy-subdivision",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "civil-windows",
   "metadata": {},
   "source": [
    "### applied on source tweet data only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "potential-speed",
   "metadata": {},
   "outputs": [],
   "source": [
    "### source tweet data only \n",
    "src_tw_df['src_sentiment']=  src_tw_df['cleaned_src_tw'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "received-lobby",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tutorial-witch",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df.label ==1]['reply_sentiment'].value_counts())\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.countplot(x='reply_sentiment',data= df[df['label'] == 1])\n",
    "\n",
    "plt.suptitle('Sentiment analysis on Reply tweets under Catergory Rumour',fontsize=16)\n",
    "plt.title('Reply tweets show more positive attitude towards rumour tweet',fontsize=12,color='grey')\n",
    "plt.savefig('graph/gurlitt/senti_reply_rumour.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sporting-target",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df.label ==0]['reply_sentiment'].value_counts())\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.countplot(x='reply_sentiment',data= df[df['label'] == 0])\n",
    "\n",
    "plt.suptitle('Sentiment analysis on Reply tweets under Catergory Non-Rumour',fontsize=16)\n",
    "plt.title('Reply tweets show more neutrual attitude towards non-rumour tweet',fontsize=12,color='grey')\n",
    "plt.savefig('graph/gurlitt/senti_reply_nonrumour.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affiliated-mining",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(src_tw_df['src_sentiment'].value_counts())\n",
    "sns.countplot(x='src_sentiment',data =src_tw_df,hue='label')\n",
    "\n",
    "plt.suptitle('Sentiment analysis on Source Tweets',fontsize=16)\n",
    "plt.title('Rumour source tweets show more neutral attitude.',fontsize=12,color='grey')\n",
    "plt.savefig('graph/gurlitt/senti_source.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "convertible-genealogy",
   "metadata": {},
   "source": [
    "### save data to csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "happy-proposal",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'D:\\論文\\PHEME9\\Data\\CSV\\gurlitt-df.csv',index=False)\n",
    "src_tw_df.to_csv(r'D:\\論文\\PHEME9\\Data\\CSV\\gurlitt-src.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "searching-rhythm",
   "metadata": {},
   "source": [
    "## Keywords Extraction\n",
    "### WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extensive-hawaii",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "charming-civilization",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "mywordcloud = WordCloud().generate(docx)\n",
    "plt.imshow(mywordcloud,interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "light-services",
   "metadata": {},
   "source": [
    "#### Reply Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-packet",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "computational-sally",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_sen = df.cleaned_reply_tw.apply(word_tokenize)\n",
    "reply_tw_list = []\n",
    "for sen in reply_sen:\n",
    "    for token in sen:\n",
    "        reply_tw_list.append(token)\n",
    "reply_tw_doc = \" \".join(reply_tw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "introductory-wrist",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "mywordcloud = WordCloud().generate(reply_tw_doc)\n",
    "plt.imshow(mywordcloud,interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "mywordcloud.to_file('graph/gurlitt/wordcloud_reply.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-permission",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive \n",
    "posi_reply_sen = df[df.reply_sentiment=='Positive'].cleaned_reply_tw.apply(word_tokenize)\n",
    "posi_reply_tw_list = []\n",
    "for sen in posi_reply_sen:\n",
    "    for token in sen:\n",
    "        posi_reply_tw_list.append(token)\n",
    "posi_reply_tw_doc = \" \".join(posi_reply_tw_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "hearing-relaxation",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "mywordcloud = WordCloud().generate(posi_reply_tw_doc)\n",
    "plt.imshow(mywordcloud,interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "mywordcloud.to_file('graph/gurlitt/wordcloud_reply_posi.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "tropical-examination",
   "metadata": {},
   "source": [
    "#### Source Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "genuine-payroll",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sen = src_tw_df.cleaned_src_tw.apply(word_tokenize)\n",
    "src_tw_list = []\n",
    "for sen in src_sen:\n",
    "    for token in sen:\n",
    "        src_tw_list.append(token)\n",
    "\n",
    "src_tw_doc = \" \".join(src_tw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-windsor",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "mywordcloud = WordCloud().generate(src_tw_doc)\n",
    "plt.imshow(mywordcloud,interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "mywordcloud.to_file('graph/gurlitt/wordcloud_src.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "blocked-singing",
   "metadata": {},
   "source": [
    "### Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vocational-charter",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "simplified-glenn",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(docx,num=30):\n",
    "    word_tokens = Counter(docx)\n",
    "    most_common = word_tokens.most_common(num)\n",
    "    result = dict(most_common)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-netherlands",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(get_tokens(reply_tw_list).items(),columns=['word','freq']).plot(kind='bar',x='word',y='freq')\n",
    "plt.title(\"Most frequent words among reply tweets are 'Prince','show','tonight'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vertical-discount",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(get_tokens(src_tw_list).items(),columns=['word','freq']).plot(kind='bar',x='word',y='freq')\n",
    "plt.title(\"Most frequent words among source tweets are 'Price', 'show', 'tonight'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "south-sydney",
   "metadata": {},
   "source": [
    "## Emotion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "substantial-discipline",
   "metadata": {},
   "source": [
    "### text2emotion package\n",
    "https://snyk.io/advisor/python/text2emotion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-vacation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import text2emotion as te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "informal-undergraduate",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = df.cleaned_reply_tw.apply(te.get_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endless-beads",
   "metadata": {},
   "outputs": [],
   "source": [
    "emo = emotion.apply(lambda x: max(x,key=x.get))\n",
    "df['reply_emotion']=emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "weekly-walter",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'D:\\論文\\PHEME9\\Data\\CSV\\gurlitt-df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "united-shield",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion1 = src_tw_df.cleaned_src_tw.apply(te.get_emotion)\n",
    "emo1 = emotion.apply(lambda x: max(x,key=x.get))\n",
    "src_tw_df['src_emotion']=emo1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "hydraulic-christmas",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tw_df.to_csv(r'D:\\論文\\PHEME9\\Data\\CSV\\gurlitt-src.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "settled-judgment",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\論文\\PHEME9\\Data\\CSV\\gurlitt-df.csv', encoding = 'utf-8', header = 0)\n",
    "src_tw_df = pd.read_csv(r'D:\\論文\\PHEME9\\Data\\CSV\\gurlitt-src.csv', encoding = 'utf-8', header =0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "silver-transcription",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tw_df.drop('emotion',axis=1,inplace=True)\n",
    "df.drop('emotion',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caroline-behavior",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
