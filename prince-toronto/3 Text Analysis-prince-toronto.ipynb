{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "suburban-optimum",
   "metadata": {},
   "source": [
    "# Text Analysis on prince-toronto data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "biblical-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datetime \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "unavailable-psychology",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r'D:\\論文\\PHEME9\\Data\\CSV\\prince-toronto.csv', encoding = 'utf-8', header = None)\n",
    "df.columns = ['src_tweet_id', 'src_user_id', 'src_tweet','src_date','reply_tweet_id','reply_user_id','reply_tweet','reply_date', 'label']\n",
    "df.reply_tweet = df.reply_tweet.replace(np.nan,\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "tamil-allocation",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_tweet_id</th>\n",
       "      <th>src_user_id</th>\n",
       "      <th>src_tweet</th>\n",
       "      <th>src_date</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>529590949136326656</td>\n",
       "      <td>842564341</td>\n",
       "      <td>As an aside, Toronto is home to @magpietoronto...</td>\n",
       "      <td>Tue Nov 04 11:08:16 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>529695075560071168</td>\n",
       "      <td>134586361</td>\n",
       "      <td>Prince loves this city. He fell for the Toront...</td>\n",
       "      <td>Tue Nov 04 18:02:01 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>529755373721632768</td>\n",
       "      <td>1395136699</td>\n",
       "      <td>PRINCE gives Toronto purple balls .....</td>\n",
       "      <td>Tue Nov 04 22:01:38 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>529761572881985536</td>\n",
       "      <td>43451286</td>\n",
       "      <td>Dammit Prince. Toronto needs a win right now.</td>\n",
       "      <td>Tue Nov 04 22:26:16 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>529399936434327552</td>\n",
       "      <td>23607802</td>\n",
       "      <td>can someone get us in 2 the secret prince show...</td>\n",
       "      <td>Mon Nov 03 22:29:15 +0000 2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>743</th>\n",
       "      <td>529778109290012673</td>\n",
       "      <td>102611790</td>\n",
       "      <td>this nigga Prince got all the unemployed middl...</td>\n",
       "      <td>Tue Nov 04 23:31:58 +0000 2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>744</th>\n",
       "      <td>529818929607475201</td>\n",
       "      <td>21137035</td>\n",
       "      <td>Live Nation wants to apologize for any inconve...</td>\n",
       "      <td>Wed Nov 05 02:14:11 +0000 2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>757</th>\n",
       "      <td>529828486975066112</td>\n",
       "      <td>20247647</td>\n",
       "      <td>#Toronto - sorry for the confusion today surro...</td>\n",
       "      <td>Wed Nov 05 02:52:09 +0000 2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>763</th>\n",
       "      <td>529830319982407680</td>\n",
       "      <td>526528241</td>\n",
       "      <td>From @DonnaGrantis on Facebook. @3RDEYEGIRL #T...</td>\n",
       "      <td>Wed Nov 05 02:59:26 +0000 2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>764</th>\n",
       "      <td>529830606210093058</td>\n",
       "      <td>291792912</td>\n",
       "      <td>PRINCE in Toronto details. Just a production r...</td>\n",
       "      <td>Wed Nov 05 03:00:34 +0000 2014</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>233 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           src_tweet_id  src_user_id  \\\n",
       "0    529590949136326656    842564341   \n",
       "1    529695075560071168    134586361   \n",
       "3    529755373721632768   1395136699   \n",
       "4    529761572881985536     43451286   \n",
       "5    529399936434327552     23607802   \n",
       "..                  ...          ...   \n",
       "743  529778109290012673    102611790   \n",
       "744  529818929607475201     21137035   \n",
       "757  529828486975066112     20247647   \n",
       "763  529830319982407680    526528241   \n",
       "764  529830606210093058    291792912   \n",
       "\n",
       "                                             src_tweet  \\\n",
       "0    As an aside, Toronto is home to @magpietoronto...   \n",
       "1    Prince loves this city. He fell for the Toront...   \n",
       "3              PRINCE gives Toronto purple balls .....   \n",
       "4        Dammit Prince. Toronto needs a win right now.   \n",
       "5    can someone get us in 2 the secret prince show...   \n",
       "..                                                 ...   \n",
       "743  this nigga Prince got all the unemployed middl...   \n",
       "744  Live Nation wants to apologize for any inconve...   \n",
       "757  #Toronto - sorry for the confusion today surro...   \n",
       "763  From @DonnaGrantis on Facebook. @3RDEYEGIRL #T...   \n",
       "764  PRINCE in Toronto details. Just a production r...   \n",
       "\n",
       "                           src_date  label  \n",
       "0    Tue Nov 04 11:08:16 +0000 2014      0  \n",
       "1    Tue Nov 04 18:02:01 +0000 2014      0  \n",
       "3    Tue Nov 04 22:01:38 +0000 2014      0  \n",
       "4    Tue Nov 04 22:26:16 +0000 2014      0  \n",
       "5    Mon Nov 03 22:29:15 +0000 2014      1  \n",
       "..                              ...    ...  \n",
       "743  Tue Nov 04 23:31:58 +0000 2014      1  \n",
       "744  Wed Nov 05 02:14:11 +0000 2014      1  \n",
       "757  Wed Nov 05 02:52:09 +0000 2014      1  \n",
       "763  Wed Nov 05 02:59:26 +0000 2014      1  \n",
       "764  Wed Nov 05 03:00:34 +0000 2014      1  \n",
       "\n",
       "[233 rows x 5 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_tw_df = df.iloc[:,[0,1,2,3,8]].copy()\n",
    "src_tw_df = src_tw_df.drop_duplicates()\n",
    "src_tw_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "approximate-declaration",
   "metadata": {},
   "source": [
    "## Prepocessing\n",
    "- Define function to process text at one time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "australian-wesley",
   "metadata": {},
   "outputs": [],
   "source": [
    "def textprocessing(text):\n",
    "    text = text.lower()\n",
    "\n",
    "    text = \" \".join([word for word in text.split() if 'http' not in word\n",
    "                                and not word.startswith('@')])\n",
    "                                #and word != 'RT'])\n",
    "    import re\n",
    "    def remove_punct(tweet):\n",
    "        new_words = []\n",
    "        for word in tweet:\n",
    "            w = re.sub(r'[^\\w\\s]','',word) #remove everything except words and space\n",
    "            w = re.sub(r'_','',w) #how to remove underscore as well\n",
    "            new_words.append(w)\n",
    "\n",
    "        return new_words\n",
    "    text = \"\".join(remove_punct(text))\n",
    "        \n",
    "    from nltk.stem.porter import PorterStemmer\n",
    "    stem = PorterStemmer()\n",
    "    \n",
    "    from nltk.tokenize import TweetTokenizer\n",
    "    tknzr = TweetTokenizer(strip_handles=True)\n",
    "    text = tknzr.tokenize(text)\n",
    "    \n",
    "    from nltk.corpus import stopwords\n",
    "    stop_words=set(stopwords.words(\"english\"))\n",
    "    def filterstopwords(tw):\n",
    "        filter_stopwords = []\n",
    "        for w in tw:\n",
    "            if w not in stop_words:\n",
    "                filter_stopwords.append(w)\n",
    "        return filter_stopwords\n",
    "    text = filterstopwords(text)\n",
    "    text = \" \".join(text)\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "detected-fellowship",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original text:  @FOHPhoto @AUXTV I need to hear chocolate rain live....\n",
      "cleaned text:  need hear chocolate rain live\n"
     ]
    }
   ],
   "source": [
    "print(\"original text: \", df.reply_tweet.iloc[20])\n",
    "print(\"cleaned text: \", textprocessing(df.reply_tweet.iloc[20]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aware-magazine",
   "metadata": {},
   "source": [
    "### applied on original data (**df**)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "continent-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_src_tw'] = df.src_tweet.apply(textprocessing)\n",
    "df['cleaned_reply_tw'] = df.reply_tweet.apply(textprocessing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "agricultural-fighter",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_tweet_id</th>\n",
       "      <th>src_user_id</th>\n",
       "      <th>src_tweet</th>\n",
       "      <th>src_date</th>\n",
       "      <th>reply_tweet_id</th>\n",
       "      <th>reply_user_id</th>\n",
       "      <th>reply_tweet</th>\n",
       "      <th>reply_date</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_src_tw</th>\n",
       "      <th>cleaned_reply_tw</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>529590949136326656</td>\n",
       "      <td>842564341</td>\n",
       "      <td>As an aside, Toronto is home to @magpietoronto...</td>\n",
       "      <td>Tue Nov 04 11:08:16 +0000 2014</td>\n",
       "      <td>5.295916e+17</td>\n",
       "      <td>1.538385e+09</td>\n",
       "      <td>@PrinceMuseum @magpietoronto @3RDEYEGIRL Lol, ...</td>\n",
       "      <td>Tue Nov 04 11:11:03 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "      <td>aside toronto home prince get lot stage clothes</td>\n",
       "      <td>lol hopefully stocking summer range 2 pop 2 au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>529695075560071168</td>\n",
       "      <td>134586361</td>\n",
       "      <td>Prince loves this city. He fell for the Toront...</td>\n",
       "      <td>Tue Nov 04 18:02:01 +0000 2014</td>\n",
       "      <td>5.296965e+17</td>\n",
       "      <td>6.211173e+07</td>\n",
       "      <td>“@ellhah: Prince loves this city. He fell for ...</td>\n",
       "      <td>Tue Nov 04 18:07:49 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "      <td>prince loves city fell toronto woman trap happ...</td>\n",
       "      <td>ellhah prince loves city fell toronto woman tr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>529695075560071168</td>\n",
       "      <td>134586361</td>\n",
       "      <td>Prince loves this city. He fell for the Toront...</td>\n",
       "      <td>Tue Nov 04 18:02:01 +0000 2014</td>\n",
       "      <td>5.296980e+17</td>\n",
       "      <td>4.464473e+07</td>\n",
       "      <td>@ellhah Prince x Red Drink Boutique invite onl...</td>\n",
       "      <td>Tue Nov 04 18:13:30 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "      <td>prince loves city fell toronto woman trap happ...</td>\n",
       "      <td>prince x red drink boutique invite party story...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>529755373721632768</td>\n",
       "      <td>1395136699</td>\n",
       "      <td>PRINCE gives Toronto purple balls .....</td>\n",
       "      <td>Tue Nov 04 22:01:38 +0000 2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>prince gives toronto purple balls</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>529761572881985536</td>\n",
       "      <td>43451286</td>\n",
       "      <td>Dammit Prince. Toronto needs a win right now.</td>\n",
       "      <td>Tue Nov 04 22:26:16 +0000 2014</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td></td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>dammit prince toronto needs win right</td>\n",
       "      <td></td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         src_tweet_id  src_user_id  \\\n",
       "0  529590949136326656    842564341   \n",
       "1  529695075560071168    134586361   \n",
       "2  529695075560071168    134586361   \n",
       "3  529755373721632768   1395136699   \n",
       "4  529761572881985536     43451286   \n",
       "\n",
       "                                           src_tweet  \\\n",
       "0  As an aside, Toronto is home to @magpietoronto...   \n",
       "1  Prince loves this city. He fell for the Toront...   \n",
       "2  Prince loves this city. He fell for the Toront...   \n",
       "3            PRINCE gives Toronto purple balls .....   \n",
       "4      Dammit Prince. Toronto needs a win right now.   \n",
       "\n",
       "                         src_date  reply_tweet_id  reply_user_id  \\\n",
       "0  Tue Nov 04 11:08:16 +0000 2014    5.295916e+17   1.538385e+09   \n",
       "1  Tue Nov 04 18:02:01 +0000 2014    5.296965e+17   6.211173e+07   \n",
       "2  Tue Nov 04 18:02:01 +0000 2014    5.296980e+17   4.464473e+07   \n",
       "3  Tue Nov 04 22:01:38 +0000 2014             NaN            NaN   \n",
       "4  Tue Nov 04 22:26:16 +0000 2014             NaN            NaN   \n",
       "\n",
       "                                         reply_tweet  \\\n",
       "0  @PrinceMuseum @magpietoronto @3RDEYEGIRL Lol, ...   \n",
       "1  “@ellhah: Prince loves this city. He fell for ...   \n",
       "2  @ellhah Prince x Red Drink Boutique invite onl...   \n",
       "3                                                      \n",
       "4                                                      \n",
       "\n",
       "                       reply_date  label  \\\n",
       "0  Tue Nov 04 11:11:03 +0000 2014      0   \n",
       "1  Tue Nov 04 18:07:49 +0000 2014      0   \n",
       "2  Tue Nov 04 18:13:30 +0000 2014      0   \n",
       "3                             NaN      0   \n",
       "4                             NaN      0   \n",
       "\n",
       "                                      cleaned_src_tw  \\\n",
       "0    aside toronto home prince get lot stage clothes   \n",
       "1  prince loves city fell toronto woman trap happ...   \n",
       "2  prince loves city fell toronto woman trap happ...   \n",
       "3                  prince gives toronto purple balls   \n",
       "4              dammit prince toronto needs win right   \n",
       "\n",
       "                                    cleaned_reply_tw  \n",
       "0  lol hopefully stocking summer range 2 pop 2 au...  \n",
       "1  ellhah prince loves city fell toronto woman tr...  \n",
       "2  prince x red drink boutique invite party story...  \n",
       "3                                                     \n",
       "4                                                     "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "center-calgary",
   "metadata": {},
   "source": [
    "### applied on source tweet data (**src_tw_df**) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "korean-price",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tw_df['cleaned_src_tw'] = src_tw_df.src_tweet.apply(textprocessing)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "disciplinary-oxide",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "tough-performance",
   "metadata": {},
   "outputs": [],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "official-heart",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment(tweet):\n",
    "    analyzer = SentimentIntensityAnalyzer()\n",
    "    vs = analyzer.polarity_scores(tweet)\n",
    "    if vs['compound'] >=0.05:\n",
    "        sentiment_label = 'Positive'\n",
    "    elif (vs['compound'] > -0.05) & (vs['compound'] < 0.05):\n",
    "        sentiment_label = 'Neutral'\n",
    "    elif vs['compound']<= -0.05:\n",
    "        sentiment_label = 'Negative' \n",
    "    result = sentiment_label\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infinite-terminology",
   "metadata": {},
   "source": [
    "### applied on original df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "better-dictionary",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['src_sentiment']= df.cleaned_src_tw.apply(get_sentiment)\n",
    "df['reply_sentiment']= df.cleaned_reply_tw.apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "vulnerable-court",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>src_tweet_id</th>\n",
       "      <th>src_user_id</th>\n",
       "      <th>src_tweet</th>\n",
       "      <th>src_date</th>\n",
       "      <th>reply_tweet_id</th>\n",
       "      <th>reply_user_id</th>\n",
       "      <th>reply_tweet</th>\n",
       "      <th>reply_date</th>\n",
       "      <th>label</th>\n",
       "      <th>cleaned_src_tw</th>\n",
       "      <th>cleaned_reply_tw</th>\n",
       "      <th>src_sentiment</th>\n",
       "      <th>reply_sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>529590949136326656</td>\n",
       "      <td>842564341</td>\n",
       "      <td>As an aside, Toronto is home to @magpietoronto...</td>\n",
       "      <td>Tue Nov 04 11:08:16 +0000 2014</td>\n",
       "      <td>5.295916e+17</td>\n",
       "      <td>1.538385e+09</td>\n",
       "      <td>@PrinceMuseum @magpietoronto @3RDEYEGIRL Lol, ...</td>\n",
       "      <td>Tue Nov 04 11:11:03 +0000 2014</td>\n",
       "      <td>0</td>\n",
       "      <td>aside toronto home prince get lot stage clothes</td>\n",
       "      <td>lol hopefully stocking summer range 2 pop 2 au...</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         src_tweet_id  src_user_id  \\\n",
       "0  529590949136326656    842564341   \n",
       "\n",
       "                                           src_tweet  \\\n",
       "0  As an aside, Toronto is home to @magpietoronto...   \n",
       "\n",
       "                         src_date  reply_tweet_id  reply_user_id  \\\n",
       "0  Tue Nov 04 11:08:16 +0000 2014    5.295916e+17   1.538385e+09   \n",
       "\n",
       "                                         reply_tweet  \\\n",
       "0  @PrinceMuseum @magpietoronto @3RDEYEGIRL Lol, ...   \n",
       "\n",
       "                       reply_date  label  \\\n",
       "0  Tue Nov 04 11:11:03 +0000 2014      0   \n",
       "\n",
       "                                    cleaned_src_tw  \\\n",
       "0  aside toronto home prince get lot stage clothes   \n",
       "\n",
       "                                    cleaned_reply_tw src_sentiment  \\\n",
       "0  lol hopefully stocking summer range 2 pop 2 au...       Neutral   \n",
       "\n",
       "  reply_sentiment  \n",
       "0        Positive  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "gentle-farming",
   "metadata": {},
   "source": [
    "### applied on source tweet data only "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "wooden-carrier",
   "metadata": {},
   "outputs": [],
   "source": [
    "### source tweet data only \n",
    "src_tw_df['src_sentiment']=  src_tw_df['cleaned_src_tw'].apply(get_sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-space",
   "metadata": {},
   "source": [
    "### Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "solved-newton",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df.label ==1]['reply_sentiment'].value_counts())\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.countplot(x='reply_sentiment',data= df[df['label'] == 1])\n",
    "\n",
    "plt.suptitle('Sentiment analysis on Reply tweets under Catergory Rumour',fontsize=16)\n",
    "plt.title('Reply tweets show more positive attitude towards rumour tweet',fontsize=12,color='grey')\n",
    "plt.savefig('graph/prince-toronto/senti_reply_rumour.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-impossible",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df[df.label ==0]['reply_sentiment'].value_counts())\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.countplot(x='reply_sentiment',data= df[df['label'] == 0])\n",
    "\n",
    "plt.suptitle('Sentiment analysis on Reply tweets under Catergory Non-Rumour',fontsize=16)\n",
    "plt.title('Reply tweets show more neutrual attitude towards non-rumour tweet',fontsize=12,color='grey')\n",
    "plt.savefig('graph/prince-toronto/senti_reply_nonrumour.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "strange-philadelphia",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(src_tw_df['src_sentiment'].value_counts())\n",
    "sns.countplot(x='src_sentiment',data =src_tw_df,hue='label')\n",
    "\n",
    "plt.suptitle('Sentiment analysis on Source Tweets',fontsize=16)\n",
    "plt.title('Rumour source tweets show more neutral attitude.',fontsize=12,color='grey')\n",
    "plt.savefig('graph/prince-toronto/senti_source.pdf',dpi=300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minimal-burning",
   "metadata": {},
   "source": [
    "### save data to csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-korean",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'D:\\論文\\PHEME9\\Data\\CSV\\prince-toronto-df.csv',index=False)\n",
    "src_tw_df.to_csv(r'D:\\論文\\PHEME9\\Data\\CSV\\prince-toronto-src.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "native-candy",
   "metadata": {},
   "source": [
    "## Keywords Extraction\n",
    "### WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rural-variable",
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "square-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "mywordcloud = WordCloud().generate(docx)\n",
    "plt.imshow(mywordcloud,interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-heart",
   "metadata": {},
   "source": [
    "#### Reply Tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustained-shopper",
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.download('punkt')\n",
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coupled-semiconductor",
   "metadata": {},
   "outputs": [],
   "source": [
    "reply_sen = df.cleaned_reply_tw.apply(word_tokenize)\n",
    "reply_tw_list = []\n",
    "for sen in reply_sen:\n",
    "    for token in sen:\n",
    "        reply_tw_list.append(token)\n",
    "reply_tw_doc = \" \".join(reply_tw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "patient-promotion",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "mywordcloud = WordCloud().generate(reply_tw_doc)\n",
    "plt.imshow(mywordcloud,interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "mywordcloud.to_file('graph/prince-toronto/wordcloud_reply.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fluid-intensity",
   "metadata": {},
   "outputs": [],
   "source": [
    "# positive \n",
    "posi_reply_sen = df[df.reply_sentiment=='Positive'].cleaned_reply_tw.apply(word_tokenize)\n",
    "posi_reply_tw_list = []\n",
    "for sen in posi_reply_sen:\n",
    "    for token in sen:\n",
    "        posi_reply_tw_list.append(token)\n",
    "posi_reply_tw_doc = \" \".join(posi_reply_tw_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "friendly-programmer",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "mywordcloud = WordCloud().generate(posi_reply_tw_doc)\n",
    "plt.imshow(mywordcloud,interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "mywordcloud.to_file('graph/prince-toronto/wordcloud_reply_posi.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regional-mileage",
   "metadata": {},
   "source": [
    "#### Source Tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tropical-robin",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_sen = src_tw_df.cleaned_src_tw.apply(word_tokenize)\n",
    "src_tw_list = []\n",
    "for sen in src_sen:\n",
    "    for token in sen:\n",
    "        src_tw_list.append(token)\n",
    "\n",
    "src_tw_doc = \" \".join(src_tw_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impossible-collins",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(20,10))\n",
    "mywordcloud = WordCloud().generate(src_tw_doc)\n",
    "plt.imshow(mywordcloud,interpolation='bilinear')\n",
    "plt.axis('off')\n",
    "plt.show()\n",
    "mywordcloud.to_file('graph/prince-toronto/wordcloud_src.pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cleared-plate",
   "metadata": {},
   "source": [
    "### Common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-reform",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fitted-somalia",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(docx,num=30):\n",
    "    word_tokens = Counter(docx)\n",
    "    most_common = word_tokens.most_common(num)\n",
    "    result = dict(most_common)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-governor",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(get_tokens(reply_tw_list).items(),columns=['word','freq']).plot(kind='bar',x='word',y='freq')\n",
    "plt.title(\"Most frequent words among reply tweets are 'Prince','show','tonight'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pretty-career",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(get_tokens(src_tw_list).items(),columns=['word','freq']).plot(kind='bar',x='word',y='freq')\n",
    "plt.title(\"Most frequent words among source tweets are 'Price', 'show', 'tonight'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "permanent-sense",
   "metadata": {},
   "source": [
    "## Emotion Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "roman-republic",
   "metadata": {},
   "source": [
    "### text2emotion package\n",
    "https://snyk.io/advisor/python/text2emotion "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "nearby-carbon",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import text2emotion as te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "annoying-albany",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion = df.cleaned_reply_tw.apply(te.get_emotion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "surrounded-diana",
   "metadata": {},
   "outputs": [],
   "source": [
    "emo = emotion.apply(lambda x: max(x,key=x.get))\n",
    "df['emotion']=emo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "capable-investment",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(r'D:\\論文\\PHEME9\\Data\\CSV\\prince-toronto-df.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "intended-qatar",
   "metadata": {},
   "outputs": [],
   "source": [
    "emotion1 = src_tw_df.cleaned_src_tw.apply(te.get_emotion)\n",
    "emo1 = emotion.apply(lambda x: max(x,key=x.get))\n",
    "src_tw_df['emotion']=emo1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "english-system",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_tw_df.to_csv(r'D:\\論文\\PHEME9\\Data\\CSV\\prince-toronto-src.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-story",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "toc-autonumbering": true,
  "toc-showmarkdowntxt": false
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
